{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHWmvbmo0Lvw",
        "outputId": "2dc196c6-8066-4476-907b-556944a8f7ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 1.8281, Train Accuracy: 35.24%\n",
            "Epoch 1/10, Validation Loss: 1.1979, Validation Accuracy: 63.70%\n",
            "Epoch 2/10, Train Loss: 1.0233, Train Accuracy: 63.97%\n",
            "Epoch 2/10, Validation Loss: 0.9287, Validation Accuracy: 70.00%\n",
            "Epoch 3/10, Train Loss: 0.7068, Train Accuracy: 75.08%\n",
            "Epoch 3/10, Validation Loss: 0.8188, Validation Accuracy: 68.52%\n",
            "Epoch 4/10, Train Loss: 0.4692, Train Accuracy: 85.24%\n",
            "Epoch 4/10, Validation Loss: 0.7756, Validation Accuracy: 72.96%\n",
            "Epoch 5/10, Train Loss: 0.3989, Train Accuracy: 88.41%\n",
            "Epoch 5/10, Validation Loss: 0.8608, Validation Accuracy: 70.74%\n",
            "Epoch 6/10, Train Loss: 0.3462, Train Accuracy: 89.52%\n",
            "Epoch 6/10, Validation Loss: 0.7932, Validation Accuracy: 74.07%\n",
            "Epoch 7/10, Train Loss: 0.3034, Train Accuracy: 89.84%\n",
            "Epoch 7/10, Validation Loss: 0.9107, Validation Accuracy: 75.19%\n",
            "Epoch 8/10, Train Loss: 0.2741, Train Accuracy: 91.27%\n",
            "Epoch 8/10, Validation Loss: 0.8904, Validation Accuracy: 74.07%\n",
            "Epoch 9/10, Train Loss: 0.2370, Train Accuracy: 92.70%\n",
            "Epoch 9/10, Validation Loss: 0.8541, Validation Accuracy: 74.81%\n",
            "Epoch 10/10, Train Loss: 0.2616, Train Accuracy: 91.59%\n",
            "Epoch 10/10, Validation Loss: 0.8504, Validation Accuracy: 75.19%\n"
          ]
        }
      ],
      "source": [
        "#!pip install ray\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from ray import tune, train\n",
        "from ray.train import Checkpoint, get_checkpoint\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import ray.cloudpickle as pickle\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.2):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.cost = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# clear any previous kaggle predictions\n",
        "kaggle_pred_dir = 'kaggle_preds'\n",
        "if os.path.isdir(kaggle_pred_dir):\n",
        "    shutil.rmtree(kaggle_pred_dir)\n",
        "os.makedirs(kaggle_pred_dir)\n",
        "\n",
        "# Load and preprocess data\n",
        "X_train = np.load('X_train.npy')\n",
        "X_val = np.load('X_test.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "y_val = np.load('y_test.npy')\n",
        "X_kaggle = np.load('X_kaggle.npy')\n",
        "kaggle_file_ids = pd.read_csv('kaggle_file_order.csv')\n",
        "\n",
        "# combine classes to create mapping from genres to integers\n",
        "y_combined = np.append(y_train, y_val, axis=0)\n",
        "class_map = dict()\n",
        "for class_idx, class_name in enumerate(np.unique(y_combined)):\n",
        "  class_map[class_name] = class_idx\n",
        "mapped_classes = np.array([class_map[value] for value in y_combined])\n",
        "y_train = mapped_classes[:len(y_train)]\n",
        "y_val = mapped_classes[len(y_train):]\n",
        "\n",
        "# Create PyTorch datasets and dataloaders\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_kaggle, dtype=torch.float32))\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "def train_mlp_kaggle(batch_size, hidden_size, dropout_rate, learning_rate, weight_decay):\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize MLP model\n",
        "    input_size = X_train.shape[1]\n",
        "    output_size = len(np.unique(y_train))  # Number of classes\n",
        "\n",
        "    model = MLP(input_size, hidden_size, output_size, dropout_rate=dropout_rate)\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = model.cost(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += targets.size(0)\n",
        "            correct_train += (predicted == targets).sum().item()\n",
        "\n",
        "        train_accuracy = correct_train / total_train\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss / len(train_loader):.4f}, Train Accuracy: {100 * train_accuracy:.2f}%\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = model.cost(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += targets.size(0)\n",
        "                correct_val += (predicted == targets).sum().item()\n",
        "\n",
        "        val_accuracy = correct_val / total_val\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {100 * val_accuracy:.2f}%\")\n",
        "\n",
        "        # simulate kaggle predictions here\n",
        "        kaggle_preds_list = []\n",
        "        with torch.no_grad():\n",
        "          for inputs in test_loader:\n",
        "            outputs = model(inputs[0])\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            kaggle_preds_list.extend(predicted.tolist())\n",
        "        for pred_idx in range(len(kaggle_preds_list)):\n",
        "          for class_name, class_idx in class_map.items():\n",
        "            if class_idx == kaggle_preds_list[pred_idx]:\n",
        "              kaggle_preds_list[pred_idx] = class_name\n",
        "              break\n",
        "        pd.concat([kaggle_file_ids, pd.DataFrame(kaggle_preds_list, columns=['class'])], axis=1).to_csv(os.path.join(kaggle_pred_dir, f'{val_accuracy:.4f}-preds.csv'), index=False)\n",
        "\n",
        "\n",
        "\n",
        "def train_mlp_raytune(config):\n",
        "\n",
        "    batch_size=config['batch_size']\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize MLP model\n",
        "    input_size = X_train.shape[1]\n",
        "    hidden_size = config['hidden_size']\n",
        "    output_size = len(np.unique(y_train))  # Number of classes\n",
        "\n",
        "    model = MLP(input_size, hidden_size, output_size, dropout_rate=config['dropout_rate'])\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "\n",
        "    checkpoint = get_checkpoint()\n",
        "    if checkpoint:\n",
        "        with checkpoint.as_directory() as checkpoint_dir:\n",
        "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
        "            with open(data_path, \"rb\") as fp:\n",
        "                checkpoint_state = pickle.load(fp)\n",
        "            start_epoch = checkpoint_state[\"epoch\"]\n",
        "            model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
        "            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    # Training loop\n",
        "    val_accuracy = 0\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = model.cost(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += targets.size(0)\n",
        "            correct_train += (predicted == targets).sum().item()\n",
        "\n",
        "        train_accuracy = correct_train / total_train\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = model.cost(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += targets.size(0)\n",
        "                correct_val += (predicted == targets).sum().item()\n",
        "\n",
        "        val_accuracy = correct_val / total_val\n",
        "\n",
        "    checkpoint_data = {\n",
        "            \"epoch\": epoch,\n",
        "            \"net_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        }\n",
        "    with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
        "        data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
        "        with open(data_path, \"wb\") as fp:\n",
        "            pickle.dump(checkpoint_data, fp)\n",
        "\n",
        "        checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
        "        train.report(\n",
        "            {\"loss\": val_loss, \"accuracy\": val_accuracy},\n",
        "            checkpoint=checkpoint,\n",
        "        )\n",
        "\n",
        "\n",
        "# uncomment the code below to search hyperparameters with raytune\n",
        "# ----------------------------------------------------------------------------\n",
        "# hyperparameter_set = {\n",
        "#     'batch_size': tune.grid_search([8, 16, 32]),\n",
        "#     'hidden_size': tune.grid_search([32, 64, 128]),\n",
        "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
        "#     'learning_rate': tune.loguniform(1e-4, 1e-1),\n",
        "#     'weight_decay': tune.loguniform(1e-6, 1e-2)\n",
        "# }\n",
        "\n",
        "# scheduler = ASHAScheduler(\n",
        "#     max_t=num_epochs,\n",
        "#     grace_period=1,\n",
        "#     reduction_factor=2)\n",
        "\n",
        "# tuner = tune.Tuner(\n",
        "#     tune.with_resources(\n",
        "#         tune.with_parameters(train_mlp_raytune),\n",
        "#         resources={\"cpu\": 1, \"gpu\": 0}\n",
        "#     ),\n",
        "#     tune_config=tune.TuneConfig(\n",
        "#         metric=\"accuracy\",\n",
        "#         mode=\"max\",\n",
        "#         scheduler=scheduler,\n",
        "#         num_samples=20,\n",
        "#     ),\n",
        "#     param_space=hyperparameter_set,\n",
        "# )\n",
        "# results = tuner.fit()\n",
        "\n",
        "# best_result = results.get_best_result(\"accuracy\", \"max\")\n",
        "\n",
        "# print(\"Best trial final validation loss: {}\".format(\n",
        "#     best_result.metrics[\"loss\"]))\n",
        "# print(\"Best trial config: {}\".format(best_result.config))\n",
        "# print(\"Best trial final validation accuracy: {}\".format(\n",
        "#     best_result.metrics[\"accuracy\"]))\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# uncomment the line below to test a particular hyperparameter configuration\n",
        "train_mlp_kaggle(8, 64, 0.22755289383709132, 0.002992940728568832, 0.006695324606709262)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "yYi4UYOOsBqQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}